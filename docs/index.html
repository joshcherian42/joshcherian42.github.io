<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Josh Cherian</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Josh Cherian</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/website_profile.png" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#publications">Publications</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#dataExploration">Projects</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="https://drive.google.com/open?id=1a8ODk1gU15M_LZBCNfH8IiCl04DraDB0">CV</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <p class="mb-0">Hi, I'm
            <span class="text-primary">Josh</span>.
          </p>
          <p id = "personal_tag">
          I just graduated with my Ph.D. in computer science, where I focused on applied machine learning. My main area of research was in the area of activity recognition, seeking to develop novel machine learning techniques designed to recognize Activities of Daily Living. I've also worked on and published in other areas including gesture recognition, sketch recognition, eye tracking, and intelligent tutoring systems. </p>
          </p>
          <div class="subheading mb-5">
            <a href="mailto:name@email.com">jcherian92@gmail.com</a>
          </div>
          <p class="mb-5"></p>
          <ul class="list-inline list-social-icons mb-0">
            <li class="list-inline-item">
            <li class="list-inline-item">
              <a href="https://www.linkedin.com/in/jcherian42/">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://github.com/jcherian42">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
            <li class="list-inline-item">
              <a href="https://scholar.google.com/citations?user=SONKHmMAAAAJ&hl=en">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-graduation-cap fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="publications">
        <div class="my-auto">
          <h2 class="mb-5">Publications</h2>
          <h3> Dissertation </h3>
          <p class="publication">
            <span style="font-weight: bold">Cherian, J.</span> <span style="font-style: italic">Towards Actualizing Real-World Human Activity Recognition for Elderly Care</span>. Texas A&M University PhD Dissertation, March 8, 2023.
          </p>
          <span>&nbsp;</span>


          <h3> Thesis </h3>
          <p class="publication">
            <span style="font-weight: bold">Cherian, J.</span> <span style="font-style: italic">Recognition of Everyday Activities through Wearable Sensors and Machine Learning</span>. Texas A&M University Master’s Thesis, December 9, 2016.
          </p>
          <div class = "publ-link">
            <input type='button' class='showabstract' id = "thesis_abstract" value='Abstract'>
            <a href = "https://oaktrust.library.tamu.edu/bitstream/handle/1969.1/161664/CHERIAN-THESIS-2017.pdf?sequence=1&isAllowed=y" target="_blank"> Paper </a>
          </div>
          <p class = 'content' id = "thesis_content"> Over the past several years, the use of wearable devices has increased dramatically,primarily for fitness monitoring, largely due to their greater sensor reliability, increasedfunctionality, smaller size, increased ease of use, and greater affordability.These deviceshave helped many people of all ages live healthier lives and achieve their personal fitness goals, as they are able to see quantifiable and graphical results of their efforts every step of the way (i.e. in real-time). Yet, while these device systems work well within the fitness domain, they have yet to achieve a convincing level of functionality in the larger domain of healthcare.<br><br>As an example, according to the Alzheimer’s Association, there are currently approximately 5.5 million Americans with Alzheimer’s Disease and approximately 5.3 million of them are over the age of 65, comprising 10% of this age group in the U.S. The economic toll of this disease is estimated to be around $259 billion. By 2050 the number of Americans with Alzheimer’s disease is predicted to reach around 16 million with an economic toll of over $1 trillion. There are other prevalent and chronic health conditions that are critically important to monitor, such as diabetes, complications from obesity, congestive heart failure, and chronic obstructive pulmonary disease (COPD) among others.<br><br>The goal of this research is to explore and develop accurate and quantifiable sensing and  machine  learning  techniques for eventual real-time health monitoring by wearable device systems. To that end, a two-tier recognition system is presented that is designed to identify health activities in a naturalistic setting based on accelerometer data of common activities. In Tier I, a traditional activity recognition approach is employed to classify short windows of data, while in Tier II, these classified windows are grouped to identify instances of a specific activity. Everyday activities that were explored in this research include brushing one’s teeth, combing one’s hair, scratching one’s chin, washing one’s hands, taking medication, and drinking. Results show that an F-measure of 0.83 is achievable when identifying these activities from each other and an F-measure of of 0.82 is achievable when identifying instances of brushing teeth over the course of a day.</p>

          <h3> Conference & Workshop Publications </h3>
          <p class="publication">
            Kim, S., <span style="font-weight: bold">Cherian, J.</span>, Ray, S., Lacy, A., Taele, P., Koh, J.I., Hammond, T. <span style="font-style: italic">A Wearable Haptic Interface for Assisting Blind and Visually Impaired Students in Learning Algebraic Equations</span>. 2023 ACM CHI Conference on Human Factors in Computing Systems Late-Breaking Work (CHI 2023 LBW), Hamburg, Germany. Apr. 23-28, 2023
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "chi23_blind_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/doi/abs/10.1145/3544549.3585815" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "chi23_blind_content"> BVI (blind and visually impaired) persons do not have the same degree of access to concepts and procedures in learning algebraic equations as sighted persons. As tactile resources are uncommon, most rely on auditory-based methods that place heavy stress on working memory. We propose a novel approach that leverages haptic technology to communicate mathematical symbols via vibrotactile patterns; this design addresses open challenges found in existing resources for BVI people and explores the potential utility of this medium for all users. We conducted three separate studies to evaluate the feasibility of our gloves that included BVI participants. The outcomes of our studies yielded insightful design discussions, including a blind participant who found the device immediately intuitive and exciting in potential.</p>


          <p class="publication">
            Williford, B., Ray, S., Koh, J.I., <span style="font-weight: bold">Cherian, J.</span>, Taele, P., Hammond, T. <span style="font-style: italic">Exploring Creativity Support for Concept Art Ideation</span>. 2023 ACM CHI Conference on Human Factors in Computing Systems Late-Breaking Work (CHI 2023 LBW), Hamburg, Germany. Apr. 23-28, 2023
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "chi23_creativity_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/doi/abs/10.1145/3544549.3585684" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "chi23_creativity_content"> Creatives often struggle with fixation on a narrow set of ideas. There is potential for co-creative systems to stimulate creatives in new and powerful ways. We sought to lay a foundation for such systems with an exploratory study. We recruited 20 university students and asked them to rapidly draw a series of creature concepts with two technology probes, one of which generates ambiguous stimuli from user strokes. We analyzed the 240 sketches visually and discovered that while most participants were fixated on humanoid forms, those that began sketching with the ambiguous stimuli first were provoked to explore more unusual varieties (p < 0.01). We also interviewed participants and used thematic analysis to analyze the data. While some participants resisted the partial loss of control and freedom, most believed the stimuli encouraged more divergent exploration and holistic thinking, and they acknowledged their potential benefit in the earliest stages of the ideation process.</p>

          <p class="publication">
            Abbasian, P., <span style="font-weight: bold">Cherian, J.</span>, Taele, P., Hammond, T. <span style="font-style: italic">Early Mild Cognitive Impairment Detection using a Hybrid Model. </span>. Companion Proceedings of the 28th International Conference on Intelligent User Interfaces (IUI '23), Sydney, Australia. Apr. 27, 2023
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "iui23_mci_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/doi/abs/10.1145/3581754.3584129" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "iui23_mci_content">Convolutional Neural Networks (CNNs) have been used in image-based applications and have made significant progress toward early detection of hard-to-detect diseases such as Mild Cognitive Impairment (MCI) and its prodromal stage Alzheimer’s Disease (AD). Despite this progress, there has been limited research on accurately distinguishing Normal Cognitive (NC) subjects from Early Mild Cognitive Impairment (EMCI) at the subject-level. This paper aims to address this gap by proposing the use of structural MRI (sMRI) images and demographic information, in conjunction with predictive models based on a shallow CNN architecture and a supervised hybrid neural network, to distinguish EMCI from NC at both the slice and subject level. These models have fewer parameters but still maintain a high level of performance in classifying EMCI and NC images and require fewer computational resources. Moreover, the model’s performance was trained and tested using only the initial and first-year visit MRI images from the newly released ADNI3 dataset.</p>

          <p class="publication">
            Koh, J.I., Ray, S., <span style="font-weight: bold">Cherian, J.</span>, Taele, P., Hammond, T. <span style="font-style: italic">An Intelligent Virtual Meeting App for Seamlessly Polling Virtual Participants "On-the-Fly" with Nonverbal Communication Cues. </span>. Companion Proceedings of the 28th International Conference on Intelligent User Interfaces (IUI '23), Sydney, Australia. Apr. 27, 2023
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "iui23_on_the_fly_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/doi/abs/10.1145/3581754.3584141" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "iui23_on_the_fly_content">The prevalence of virtual meeting software and webcams have normalized holding group meetings both remotely and distributed. However, virtual meetings reduce viewing to computing displays and listening to unidirectional speakers, which disrupt real-life social cues for activities such as informally polling participants “on-the-fly”. This paper proposes an intelligent virtual meetings app called Show of Hands, which leverages nonverbal communication cues to spontaneously poll virtual participants. The app recognizes virtual participants’ intuitive real-time hand gestures to express intended polling selections, and then displays in real-time highly-visible video filters that overlay participants’ camera views and a visual chart of the aggregated polling counts. Our work benefits virtual participants in seamlessly conducting spontaneous polls to gauge opinions or check knowledge of attendees without prior preparation, expressing poll responses using familiar physical gestures, and being better informed of poll results with both a distributed view of video filters and a focused chart visualization.</p>


          <p class="publication">
            Maity, S., Taele, P., <span style="font-weight: bold">Cherian, J.</span>, Ray, S., Koh, J.I., Hammond, T. <span style="font-style: italic">Chemisketch: Learning Lewis Dot Diagrams with Pen-Based Editing Interactions and Immediate Feedback</span>. Companion Proceedings of the 28th International Conference on Intelligent User Interfaces (IUI '23), Sydney, Australia. Apr. 27, 2023
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "iui23_chemisketch_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/doi/abs/10.1145/3581754.3584146" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "iui23_chemisketch_content"> Introductory chemistry courses teach the process of drawing basic chemical molecules with the use of Lewis dot diagrams. However, many beginner students struggle in mastering these diagrams. While there exists educational applications that focus on assisting students in learning Lewis dot diagrams, these applications do not leverage students’ pen-and-paper activities in practicing their usage of Lewis dot diagrams. Furthermore, there exists automated sketch recognition approaches that focus on classifying advanced chemical diagrams, but such approaches focus more on robustly recognizing experts’ completed diagrams and less on assessing novices’ practice diagrams. In this study, we propose an intelligent tutoring system for enabling students to practice constructing Lewis dot diagrams (i.e., constructing, checking, and modifying) with automatic constructive feedback, while allowing them to retain their familiar writing conventions similarly to pen and paper.</p>

          <p class="publication">
            Koh, J.I., Ray, S., <span style="font-weight: bold">Cherian, J.</span>, Taele, P., Hammond, T. <span style="font-style: italic">Show of Hands: Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions</span>. 27th International Conference on Intelligent User Interfaces (IUI '22), Helsinki, Finland. Mar. 22-25, 2022
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "iui22_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/doi/10.1145/3490099.3511153" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "iui22_content"> Increased virtual meeting software usage has allowed people to meet remotely in a more seamless fashion. However, compared to in-person meetings, valuable interaction cues such as impromptu group polling are less optimally executed due to increased difficulty in gauging remote participants, while also requiring prior meeting setup for automated counting with built-in polling tools. We propose a novel intelligent user interface approach for virtual meeting software that supports impromptu polling interactions by leveraging real-time hand gesture recognition and video filter feedback. We conducted studies to design and evaluate this intuitive gesture-based polling system with visual feedback. Our results demonstrated that our system was able to recognize attendees’ gestures and poll responses with reasonable accuracy, and showed improvements in hosts’ task workload performance. From our findings, our interface informs hosts of valuable results while maintaining organic gestural interaction cues with attendees similar to in-person meetings.</p>

          <p class="publication">
            Leland J., Stanfill, E., <span style="font-weight: bold">Cherian, J.</span>, Hammond, T. <span style="font-style: italic">Recognizing Seatbelt-Fastening Activity with Wearable Technology and Machine Learning</span>. 2021 ACM CHI Conference on Human Factors in Computing Systems Late-Breaking Work (CHI 2021 LBW), Yokohama, Japan. May. 8-13, 2021
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "chi21_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/doi/abs/10.1145/3411763.3451705" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "chi21_content"> Nearly 1.35 million people are killed in automobile accidents every year, and nearly half of all individuals involved in these accidents were not wearing their seatbelt at the time of the crash. This lack of safety precaution occurs in spite of the numerous safety sensors and warning indicators embedded within modern vehicles. This presents a clear need for more effective methods of encouraging consistent seatbelt use. To that end, this work leverages wearable technology and activity recognition techniques to detect when individuals have buckled their seatbelt. To develop such a system, we collected smartwatch data from 26 different users. From this data, we identified trends which inspired the development of novel features. Using these features, we trained models to identify the motion of fastening a seatbelt in real-time. This model serves as the basis for future work in which systems can provide personalized and effective interventions to ensure seatbelt use.</p>

          <p class="publication">
            <span style="font-weight: bold">Cherian, J.</span>, Ray, S., Hammond, T. <span style="font-style: italic">An Activity Recognition System for Taking Medicine Using In-The-Wild Data to Promote Medication Adherence</span>. 26th International Conference on Intelligent User Interfaces (IUI '21), College Station, Texas, USA. Apr. 13-17, 2021
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "iui21_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/doi/abs/10.1145/3397481.3450673" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "iui21_content"> Nearly half of people prescribed medication to treat chronic or short-term conditions do not take their medicine as prescribed. This leads to worse treatment outcomes, higher hospital admission rates, increased healthcare costs, and increased morbidity and mortality rates. While some instances of medication non-adherence are a result of problems with the treatment plan or barriers caused by the health care provider, many are instances caused by patient-related factors such as forgetting, running out of medication, and not understanding the required dosages. This presents a clear need for patient-centered systems that can reliably increase medication adherence. To that end, in this work we describe an activity recognition system capable of recognizing when individuals take medication in an unconstrained, real-world environment. Our methodology uses a modified version of the Bagging ensemble method to suit unbalanced data and a classifier trained on the prediction probabilities of the Bagging classifier to identify when individuals took medication during a full-day study. Using this methodology we are able to recognize when individuals took medication with an F-measure of 0.77. Our system is a first step towards developing personal health interfaces that are capable of providing personalized medication adherence interventions.</p>

          <p class="publication">
            Williford, B., Runyon, M., <span style="font-weight: bold">Cherian, J.</span>, Li, W.K., Linsey, J., Hammond, T. <span style="font-style: italic"> A Framework for Motivating Sketching Practice with Sketch-based Gameplay</span>. CHI PLAY'19, Barcelona, Spain, Oct. 22-25, 2019
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "chiplay19_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/doi/abs/10.1145/3311350.3347175" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "chiplay19_content"> Sketching is a valuable skill to learn but requires extensive motivation and practice to improve. We present a framework for motivating practice with sketch-based gameplay that is rooted in a grounded theory study of the motivations of various individuals with different skills levels. The individuals interviewed included a range from novice and intermediate industrial design students to established design professionals. Four categories emerged that explain the differences in motivation between individuals with different skill levels, including achievement, competition, communication, and creativity. We also present a case study of the implementation of two different gameplay approaches for encouraging line work practice in a high school art course and a university sketching course. The study revealed that both approaches were very engaging and motivating to students, with 72,842 lines practiced across the 150 students overall. We also gained insights about how the approaches differed in motivating students, and share principles we learned on motivating students with gameplay that may be useful to other researchers, educators, and technologists.</p>

          <p class="publication">
            Mendiola, V., Doss, A., Adams, W., Ramos, J., Bruns, M., <span style="font-weight: bold">Cherian, J.</span>, Kohli, P., Goldberg, D., Hammond, T. <span style="font-style: italic"> Automatic Exercise Recognition with Machine Learning</span>. 33rd AAAI Conference on Artificial Intelligence International Workshop on Health Intelligence (W3PHAI'19), Honolulu, Hawaii, USA. Jan. 27-Feb. 1, 2019
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "aaai19_abstract" value='Abstract'>
            <a href = "https://drive.google.com/file/d/175PlZttCUZVuh4uAwPfdhl_ofLYchcP9/view?usp=sharing" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "aaai19_content"> Although most individuals understand the importance of regular physical activity, many still lead mostly sedentary lives. The use of smartphones and fitness trackers has mitigated this trend some, as individuals are able to track their physical activity; however, these devices are still unable to reliably recognize many common exercises. To that end, we propose a system designed to recognize sit ups, bench presses, bicep curls, squats, and shoulder presses using accelerometer data from a smartwatch. Additionally, we evaluate the effectiveness of this recognition in a real-time setting by developing and testing a smart- phone application built on top of this system. Our system recognized these activities with overall F-measures of 0.94 and 0.87 in a controlled environment and real-time setting respectively. Both users who were and who were not regularly physically active responded positively to our system, noting that our system would encourage them to start or continue exercising regularly.</p>

          <p class="publication">
            <span style="font-weight: bold">Cherian, J</span>.<span style="font-style: italic"> Automatic Recognition of Hygiene Activities and Personalized Interventions for Chronic Care</span>. 23rd International Conference on Intelligent User Interfaces (IUI '18), Tokyo, Japan. Mar. 7-11, 2018
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "iui_abstract" value='Abstract'>
            <a href = "https://drive.google.com/file/d/1vuKnPVtxoEsT0zVfDsrHsrQWmwuy4tKm/view?usp=sharing" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "iui_content"> The number of individuals living with chronic conditions continues to rise. As a result, a significant emphasis has been placed on both improving their quality of life as well as decreasing the cost and burden of caring for them. One particularly promising avenue for achieving this is the use of wearable devices, as they have become both affordable and reliable in recognizing fitness activities. However, while the existing algorithms reliably recognize physically intensive activities (e.g., walking vs. swimming), they fail to recognize personal hygiene actives that have more subtle differences (e.g., brushing teeth vs. washing hands). This research aims to develop novel features and intelligent, multi-stage algorithms that can reliably recognize such personal hygiene activities for chronic care. Additionally, we aim to further supplement this activity recognition with personalized interventions that enable individuals to manage their own personal health.</p>

          <p class="publication">
            Bauman, B., Gunhouse, R., Jones, A., Da Silva, W., Sharar, S., Rajanna, V., <span style="font-weight: bold">Cherian, J.</span>, Koh, J., Hammond, T. <span style="font-style: italic">VisualEYEze: A Web-based Solution for Receiving Feedback on Artwork Through Eye Tracking.</span> 23rd International Conference on Intelligent User Interfaces Workshop on Web Intelligence and Interaction (WII 2018), Tokyo, Japan. Mar. 7-11, 2018
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "visualeyeze_abstract" value='Abstract'>
            <a href = "https://drive.google.com/file/d/1KDl7schKFlV02tWH3isV-2Y6-b9WJZHj/view?usp=sharing" target="_blank" style = "margin-right: 1rem"> Paper </a>
          </div>
          <p class = 'content' id = "visualeyeze_content"> Artists value the ability to determine what parts of their composition is most appreciated by viewers. This information normally comes straight from viewers in the form of oral and written feedback; however, due to the lack of participation on the viewers part and because much of our visual understanding of artwork can be subconscious and difficult to express verbally, the value of this feedback is limited. Eye tracking technology has been used before to analyze artwork, however, most of this work has been performed in a controlled lab setting and as such this technology remains largely inaccessible to individual artists who may seek feedback. To address this issue, we developed a web-based system where artists can upload their artwork to be viewed by the viewers on their computer while a web camera tracks their eye movements. The artist receives feedback in the form of visualized eye tracking data that depicts what areas on the image looked at the most by viewers. We evaluated our system by having 5 artists upload a total of 17 images, which were subsequently viewed by 20 users. The artists expressed that seeing eye tracking data visualized on their artwork indicating the areas of interest is a unique way of receiving feedback and is highly useful. Also, they felt that the platform makes the artists more aware of their compositions; something that can especially help inexperienced artists. Furthermore, 90% of the viewers expressed that they were comfortable in providing eye movement data as a form of feedback to the artists.</p>

          <p class="publication">
            <span style="font-weight: bold">Cherian, J.</span> and Hammond, T.<span style="font-style: italic"> A Two Tier Recognition System for Recognizing Brushing Teeth</span>. Richard Tapia Celebration of Diversity in Computing Conference 2017 (Tapia '17), Atlanta, Georgia. Sept. 20-23, 2017
          </p>
          <div class = "publ-link">
            <a href = "https://drive.google.com/file/d/10uDE-rEeGE_st_A1x6jI4v82Ndwjatf6/view?usp=sharing" target="_blank" style = "margin-right: 1rem"> Paper </a>
            <a href = "https://drive.google.com/file/d/1K17ObzRNQOP9IguqGvxLINjCm-aIFxSf/view?usp=sharing" target="_blank"> Poster </a>
          </div>

          <p class="publication">
            <span style="font-weight: bold">Cherian, J.</span>, Rajanna, V., Goldberg, D., and Hammond, T. <span style="font-style: italic">Did you Remember to Brush? A Noninvasive Wearable Approachto Recognizing Brushing Teeth for Elderly Care</span>. 11th EAI International Conference on Pervasive Computing Technologiesfor Healthcare. Barcelona, Spain. May 23–26, 2017
          </p>
          <div class = "publ-link">
            <input type='button' class = 'showabstract' id = "pervasive_abstract" value='Abstract'>
            <a href = "https://dl.acm.org/citation.cfm?id=3154866" target="_blank"> Paper </a>
          </div>
          <p class = 'content' id = "pervasive_content"> Failing to brush one’s teeth regularly can have surprisingly serious health  consequences, from periodontal disease to coronary heart disease to pancreatic cancer. This problem is especially worrying when caring for the elderly and/or individuals with dementia, as they often forget or are unable to perform standard health activities such as brushing their teeth, washing their hands, and taking medication. To ensure that such individuals are correctly looked after they are placed under the supervision of caretakers or family members, simultaneously limiting their independence and placing an immense burden on their family members and caretakers. To address this problem we developed a non-invasive wearable system based on a wrist-mounted accelerometer to accurately identify when a person brushed their teeth. We tested the efficacy of our system with a month-long in-the-wild study and achieved an accuracy of 94% and an F-measure of 0.82.</p>

          <p class="publication">
            <span style="font-weight: bold">Cherian, J.</span>, Goldberg, D., and Hammond, T. <span style="font-style: italic">Sensing Day-to-Day Activities through Wearable Sensors and AI</span>. 2016 Workshop on Analysis of Movement Data (GIScience 2016), Montreal, Canada. Sept. 27, 2016
          </p>
          <div class = "publ-link">
            <a href = "https://drive.google.com/file/d/1TmNqAOxAdkwqiTpm8xGGg779KlD4SjGx/view?usp=sharing" target="_blank"> Paper </a>
          </div>

          <h3>Journal Publications</h3>
          <p class="publication">
            Petersheim, C., Lahey, J., <span style="font-weight: bold">Cherian, J.</span>, Pina, A., Alexander, G., Hammond, T. <span style="font-style: italic">Comparing Student and Recruiter Evaluations of Computer Science Resumes</span>. IEEE Transactions on Education, August 2022
          </p>
          <div class = "publ-link">
              <input type='button' class = 'showabstract' id = "ieee_te22_abstract" value='Abstract'>
              <a href = "https://ieeexplore.ieee.org/abstract/document/9873810" target="_blank"> Paper </a>
          </div>
          <p class = 'content' id = "ieee_te22_content"> Contribution: This study identifies which entry-level computer science (CS) resume items are most important and compares the ratings of student and recruiter participants to investigate the accuracy of student beliefs. To the authors’ knowledge, this study is the first to analyze the extent to which CS students understand the resume screening process. The results of this have consequences for students in their own resume development. Background: Although prior research studies the importance of different resume items generally, little is known about resumes for CS majors, which may contain distinctive sections. Less still is known about whether students understand the resume screening process. Research Questions: Which items on entry-level CS resumes most directly influence screening decisions? What gaps exist between CS students’ and recruiters’ perceptions of resume items? Does the disparity in expertise between students and recruiters contribute to differences in resume screening? Methodology: 197 recruiters and 73 CS undergraduates screened randomized CS resumes. Data were analyzed using ordinary least-squares regression with interactions. Findings: Students were more likely than recruiters to move resumes to the next level and spent about 7 s less than recruiters when screening resumes. Though students correctly assessed the value of most resume items, they significantly overestimated the value of prior work experience such as internships.</p>

          <p class="publication">
            Edey, D., Hillin, J., Thompson, C.M., <span style="font-weight: bold">Cherian, J.</span>, Hammond, T. <span style="font-style: italic">Closing the Gender Gap in Natural Hazards Education for Young Adults</span>. Journal for STEM Education Research, March 2022
          </p>
          <div class = "publ-link">
              <input type='button' class = 'showabstract' id = "stem21_abstract" value='Abstract'>
              <a href = "https://link.springer.com/article/10.1007/s41979-022-00068-4" target="_blank"> Paper </a>
          </div>
          <p class = 'content' id = "stem21_content"> Different people react differently to disasters, hazards, and risks depending on how they view those risks. Various factors can influence these risk perceptions, including demographic characteristics, such as gender and previous experience. Disaster education (DE) has been identified as a method to positively influence risk perception and reduce disaster vulnerability and impacts at various scales. Gender has been identified as an influencing factor on risk perception and educational outcomes in STEM subjects in previous studies, but the role gender plays in a formal DE program has not previously been evaluated. This study utilizes a formal online DE curriculum program to evaluate genders’ role in young adults’ knowledge and risk perceptions of natural hazards before and after DE curriculum completion. Statistical analysis t-tests indicated that women scored significantly lower than men in individual and total module pre-tests. However, this gender gap was closed in total module post-scores. Women overall had consistently higher risk perceptions according to both t-tests (p < 0.05) and regression models, but both genders had increased risk perceptions (p < 0.05) and overall program scores at the end of the course (p < 0.001). Women also started and ended the course with greater feelings of fear than men (p < 0.05), whereas men felt more knowledgeable about hazards than women (p < 0.05). Statistically significant differences between the two genders were identified for pre-curriculum scores, and gender had a strong, consistent influence on risk perception. Women were also found to have higher risk perceptions before and after the course, but both genders experienced an increase in risk perception scores. The DE course increased awareness and closed the knowledge gap between genders.</p>

          <p class="publication">
            Edey, D., Thompson, C.M., <span style="font-weight: bold">Cherian, J.</span>, Hammond, T. <span style="font-style: italic">Online Local Natural Hazards Education for Young Adults: Assessing Program Efficacy and Changes in Risk Perceptions for Texas natural Hazards</span>. Journal of Geography in Higher Education, January 2021
          </p>
          <div class = "publ-link">
              <input type='button' class = 'showabstract' id = "geog21_abstract" value='Abstract'>
              <a href = "https://www.tandfonline.com/doi/full/10.1080/03098265.2021.1947204" target="_blank"> Paper </a>
          </div>
          <p class = 'content' id = "geog21_content"> Risk perceptions can influence how people prepare, react, and respond to a natural hazard. For dependent populations with increased vulnerability, such as youth, adolescents, and young adults, hazard education programs can influence their risk perceptions and increase their hazard awareness. However, most school curriculum does not include in-depth formal natural hazards education; Texas curriculum in secondary schools, for example, does not typically cover local natural hazards or their impacts in-depth due to other curriculum priorities. Thus, students may not be receiving potentially life-saving information on their local natural hazards or how to plan for or respond to disasters. In response, this research presents a formal, online, and youth-centric natural hazard educational (NHE) curriculum that examines the curriculum’s effects on risk perception and subject matter proficiency using a local pilot study with college students as proxies for high school adolescents in Texas. Results suggest that the curriculum content improved overall natural hazards knowledge in participants (p < 0.01) and that participants with higher post-curriculum scores demonstrated higher risk perception and hazard awareness. These findings demonstrate how exposure to natural hazards educational programs can increase hazard awareness and coping capacity in young adults and adolescents</p>

          <p class="publication">
            Koh, J., <span style="font-weight: bold">Cherian, J.</span>, Taele, P., Hammond, T. <span style="font-style: italic">Developing a Hand Gesture Recognition System for Mapping Symbolic Hand Gestures to Analogous Emoji in Computer-Mediated Communication</span>. ACM Transactions on Interactive Intelligent Systems (TiiS), February 2019
          </p>
          <div class = "publ-link">
              <input type='button' class = 'showabstract' id = "tiis_emoji_abstract" value='Abstract'>
              <a href = "https://dl.acm.org/doi/abs/10.1145/3297277" target="_blank"> Paper </a>
          </div>
          <p class = 'content' id = "tiis_emoji_content"> Recent trends in computer-mediated communications (CMC) have not only led to expanded instant messaging through the use of images and videos, but have also expanded traditional text messaging with richer content in the form of visual communication markers (VCM) such as emoticons, emojis, and stickers. VCMs could prevent a potential loss of subtle emotional conversation in CMC, which is delivered by nonverbal cues that convey affective and emotional information. However, as the number of VCMs grows in the selection set, the problem of VCM entry needs to be addressed. Furthermore, conventional means of accessing VCMs continue to rely on input entry methods that are not directly and intimately tied to expressive nonverbal cues. In this work, we aim to address this issue, by facilitating the use of an alternative form of VCM entry: hand gestures. To that end, we propose a user-defined hand gesture set that is highly representative of a number of VCMs and a two-stage hand gesture recognition system (trajectory-based, shape-based) that can identify these user-defined hand gestures with an accuracy of 82%. By developing such a system, we aim to allow people using low-bandwidth forms of CMCs to still enjoy their convenient and discreet properties, while also allowing them to experience more of the intimacy and expressiveness of higher-bandwidth online communication.</p>

          <p class="publication">
            Hammond, T., Kumar, S., Runyon M., <span style="font-weight: bold">Cherian, J.</span>, Williford, B., Keshavabhotla, S., Li, W., Linsey, J. <span style="font-style: italic">It’s Not Just About Accuracy: Metrics That Matter When Measuring Expert Sketching Ability</span>. ACM Transactions on Interactive Intelligent Systems (TiiS). July 2018
          </p>
          <div class = "publ-link">
              <input type='button' class = 'showabstract' id = "tiis_acc_abstract" value='Abstract'>
              <a href = "https://dl.acm.org/citation.cfm?id=3181673" target="_blank"> Paper </a>
          </div>
          <p class = 'content' id = "tiis_acc_content"> Design sketching is an important skill for designers, engineers, and creative professionals as it allows them to express their ideas and concepts in a visual medium. Being a critical and versatile skill for many different disciplines, courses on design sketching are often taught in universities. Courses today predominately rely on pen and paper; however, this traditional pedagogy is limited by the availability of human instructors who can provide personalized feedback. Using a stylus-based intelligent tutoring system called SketchTivity, we aim to eventually mimic the feedback given by an instructor and assess student-drawn sketches to give students insight into areas for improvement.<br> <br>In order to provide effective feedback to users, it is important to identify what aspects of their sketches they should work on to improve their sketching ability. After consulting with several domain experts in sketching, we came up with several classes of features that could potentially differentiate expert and novice sketches. Because improvement on one metric, such as speed, may result in a decrease in another metric, such as accuracy, the creation of a single score may not mean much to the user. We attempted to create a single internal score that represents overall drawing skill so that the system can track improvement over time, and found that this score correlates highly with expert rankings. We gathered over 2000 sketches from 20 novices and four experts for analysis. We identified key metrics for quality assessment that were shown to significantly correlate with the quality of expert sketches and provide insight into providing intelligent user feedback in the future.</p>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="dataExploration">
        <div class="my-auto">
          <h2 class="mb-5">Projects</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Active Living Austin</h3>
              <img class="project-img" alt="ALA Website" src="img/ALA_Website_home.png">
            </div>
            <div class="resume-date text-md-right">
              <p class="project-desc">This study aimed to understand how activity-friendly communities can impact the level of physical activity among residents. As a computer science researcher on this project, I developed a machine learning pipeline capable of identifying the physical activity of study participants from their accelerometer and GPS data. I also designed and developed a website that allowed the non-computer science researchers on the project to upload newly collected sensor data and obtain the classification results. </p>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">FDAnalyzer</h3>
              <!-- <a href="https://flask-datamining.herokuapp.com/visualization"> -->
                 <img class="project-img" alt="FDAnalyzer" src="img/FDAnalyzer.gif">
              <!-- </a> -->
              <!-- <p style="font-size:xx-small;">Click to interact</p> -->
            </div>
            <div class="resume-date text-md-right">
              <p class="project-desc">This was a data visualization tool developed to visualize the <a id='faers' href="https://open.fda.gov/data/faers/">FDA Adverse Event Reporting System (FAERS)</a> dataset. </p>
              <p class="project-desc">I was responsible for all of the front-end work.</p>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Startups Visualization</h3>
              <a href="https://jcherian42.github.io/startups-visualization/">
                 <img class="project-img" alt="Startup Visualization" src="img/startupvisualization.gif">
              </a>
              <p style="font-size:xx-small;">Click to interact</p>
            </div>
            <div class="resume-date text-md-right">
              <p class="project-desc">This was a data visualization tool developed to visualize startup information from a variety of sources. </p>
              <p class="project-desc">I was responsible for all of the front-end work.</p>
            </div>
          </div>

        </div>
      </section>

    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.js"></script>

  </body>

</html>
